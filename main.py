import os
import json
import pdfplumber
from typing import List, Tuple
import csv

from src.resume_ranker.interface import Ranker


def read_text_file(path: str) -> str:
    """è¯»å– UTF-8 æ–‡æœ¬æ–‡ä»¶"""
    with open(path, "r", encoding="utf-8") as f:
        return f.read().strip()


def read_pdf_file(path: str) -> Tuple[str, int]:
    """è¯»å– PDF æ–‡ä»¶æ–‡æœ¬ï¼Œè¿”å› (text, page_count)"""
    pages = 0
    text_chunks: List[str] = []
    with pdfplumber.open(path) as pdf:
        pages = len(pdf.pages)
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text_chunks.append(page_text)
    text = "\n".join(text_chunks).strip()
    return text, pages


def load_job_description(jd_file: str) -> str:
    """è¯»å–èŒä½æè¿° txt æ–‡ä»¶"""
    if not os.path.isfile(jd_file):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°èŒä½æè¿°æ–‡ä»¶: {jd_file}")
    return read_text_file(jd_file)


def load_resumes(resumes_dir: str) -> Tuple[List[str], List[str], List[int]]:
    """è¯»å–ç›®å½•ä¸‹æ‰€æœ‰ PDF ç®€å†ï¼Œè¿”å› (texts, file_names, page_counts)"""
    texts: List[str] = []
    names: List[str] = []
    pages: List[int] = []
    if not os.path.isdir(resumes_dir):
        raise FileNotFoundError(f"ç®€å†ç›®å½•ä¸å­˜åœ¨: {resumes_dir}")
    for fname in sorted(os.listdir(resumes_dir)):
        if fname.lower().endswith(".pdf"):
            fpath = os.path.join(resumes_dir, fname)
            txt, pg = read_pdf_file(fpath)
            texts.append(txt)
            names.append(fname)
            pages.append(pg)
    if not texts:
        raise FileNotFoundError(f"åœ¨ {resumes_dir} ä¸­æœªæ‰¾åˆ° PDF ç®€å†")
    return texts, names, pages


def _np_to_native(o):
    """å°† numpy æ ‡é‡/æ•°ç»„å®‰å…¨è½¬æ¢ä¸ºåŸç”Ÿ Python ç±»å‹ï¼Œé¿å… json åºåˆ—åŒ–é”™è¯¯"""
    try:
        import numpy as np
        if isinstance(o, (np.integer,)):
            return int(o)
        if isinstance(o, (np.floating,)):
            return float(o)
        if isinstance(o, (np.ndarray,)):
            return o.tolist()
    except Exception:
        pass
    return o


def main():
    # ===== é…ç½®è·¯å¾„ =====
    jd_file = "./data/job_description/jd1.txt"  # èŒä½æè¿°æ–‡ä»¶è·¯å¾„
    resumes_dir = "./data/resumes"              # ç®€å† PDF æ–‡ä»¶å¤¹è·¯å¾„
    cache_dir = os.environ.get("HF_HOME", None) # æ¨¡å‹ç¼“å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰

    # ===== åŠ è½½æ•°æ® =====
    job_desc = load_job_description(jd_file)
    resumes, resume_names, resume_pages = load_resumes(resumes_dir)


    # # ===== CSV è¾“å‡ºå‡†å¤‡ =====
    # blend_values = [None, 0.0, 0.3, 0.5, 0.7, 1.0]
    # output_rows = []

    # # åˆ›å»ºä¸€æ¬¡ Rankerï¼Œé¿å…æ¯æ¬¡å¾ªç¯éƒ½é‡æ–°åŠ è½½æ¨¡å‹
    # ranker = Ranker(model_cache_dir=cache_dir, device="cpu")
    # ranker.load_models()

    # for alpha in blend_values:
    #     results = ranker.rank(
    #         job_desc,
    #         resumes,
    #         top_k=min(30, len(resumes)),
    #         cross_rerank_k=min(30, len(resumes)),
    #         blend_alpha=alpha# alpha * biencoder + (1 - alpha) * cross_encoder
    #     )

    #     for i, r in enumerate(results):
    #         idx = int(r.resume_index)
    #         fname = resume_names[idx] if 0 <= idx < len(resume_names) else f"idx_{idx}"
    #         output_rows.append({
    #             "blend_alpha": alpha if alpha is not None else "cross_only",
    #             "rank": i + 1,
    #             "file": fname,
    #             "score": float(r.score),
    #             "bi_score": float(r.bi_score) if r.bi_score is not None else None,
    #             "cross_score": float(r.cross_score) if r.cross_score is not None else None,
    #             "final_score": float(r.final_score) if r.final_score is not None else None,
    #             "pages": int(resume_pages[idx]) if 0 <= idx < len(resume_pages) else None,
    #             "meta": r.meta
    #         })

    # # ===== å†™å…¥ CSV =====
    # csv_file = os.path.join(os.getcwd(), "ranking_results.csv")
    # with open(csv_file, mode="w", newline="", encoding="utf-8") as f:
    #     writer = csv.DictWriter(f, fieldnames=[
    #         "blend_alpha", "rank", "file", "score", "bi_score", "cross_score", "final_score", "pages", "meta"
    #     ])
    #     writer.writeheader()
    #     writer.writerows(output_rows)

    # print(f"âœ… æ’åç»“æœå·²ä¿å­˜åˆ°: {csv_file}")
    # ===== æ’åº =====
    ranker = Ranker(
        model_cache_dir=cache_dir,
        device="cpu"
    )
    print("ğŸ” Using Bi-Encoder model:", ranker.bi.model_name)
    if ranker.cross:
        print("ğŸ” Using Cross-Encoder model:", ranker.cross.model_name)
    else:
        print("âš  No Cross-Encoder loaded")

    ranker.load_models()
    results = ranker.rank(
        job_desc,
        resumes,
        top_k=min(30, len(resumes)),
        cross_rerank_k=min(15, len(resumes)),
        blend_alpha=0.5
    )

    # ===== è¾“å‡ºç»“æœï¼ˆåŒ…å«æ–‡ä»¶åï¼›æ•°å€¼å¼ºåˆ¶è½¬åŸç”Ÿï¼‰ =====
    payload = []
    for i, r in enumerate(results):
        idx = int(r.resume_index)
        fname = resume_names[idx] if 0 <= idx < len(resume_names) else f"idx_{idx}"
        payload.append({
            "rank": int(i + 1),
            "file": fname,
            "score": float(r.score),               # å…¼å®¹å­—æ®µï¼ˆfinal_scoreï¼‰
            "final_score": float(r.final_score) if r.final_score is not None else None,
            "bi_score": float(r.bi_score) if r.bi_score is not None else None,
            "cross_score": float(r.cross_score) if r.cross_score is not None else None,
            "resume_index": int(r.resume_index),
            "pages": int(resume_pages[idx]) if 0 <= idx < len(resume_pages) else None,
            "meta": r.meta
        })

    print(json.dumps(payload, ensure_ascii=False, indent=2, default=_np_to_native))



if __name__ == "__main__":
    print("ğŸ” Starting Resume Ranker...")
    main()
