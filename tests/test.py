# src/resume_ranker/main.py
import os
import json
import pdfplumber
from typing import List, Tuple
from src.resume_ranker.interface import Ranker
from src.resume_ranker.agents.providers.openai_provider import OpenAIProvider
from src.resume_ranker.agents.prompts import RESUME_EXTRACTION_PROMPT, JD_EXTRACTION_PROMPT


def read_text_file(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read().strip()


def read_pdf_file(path: str) -> Tuple[str, int]:
    text_chunks: List[str] = []
    with pdfplumber.open(path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text_chunks.append(page_text)
    text = "\n".join(text_chunks).strip()
    return text, len(pdf.pages)


def load_job_description(jd_file: str) -> str:
    if not os.path.isfile(jd_file):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°èŒä½æè¿°æ–‡ä»¶: {jd_file}")
    return read_text_file(jd_file)


def load_resumes(resumes_dir: str) -> Tuple[List[str], List[str], List[int]]:
    texts, names, pages = [], [], []
    if not os.path.isdir(resumes_dir):
        raise FileNotFoundError(f"ç®€å†ç›®å½•ä¸å­˜åœ¨: {resumes_dir}")
    for fname in sorted(os.listdir(resumes_dir)):
        if fname.lower().endswith(".pdf"):
            fpath = os.path.join(resumes_dir, fname)
            txt, pg = read_pdf_file(fpath)
            texts.append(txt)
            names.append(fname)
            pages.append(pg)
    if not texts:
        raise FileNotFoundError(f"åœ¨ {resumes_dir} ä¸­æœªæ‰¾åˆ° PDF ç®€å†")
    return texts, names, pages


def extract_with_openai(provider: OpenAIProvider, text: str, prompt_template: str, kind: str):
    prompt = prompt_template.format(resume_text=text) if kind == "resume" else prompt_template.format(jd_text=text)
    raw = provider.generate(prompt)
    try:
        return json.loads(raw)
    except Exception:
        return {"raw_output": raw}

def serialize_resume_json(resume: dict) -> str:
    parts = []
    if resume.get("title"):
        parts.append(f"Title: {resume['title']}")
    if resume.get("skills"):
        parts.append(f"Skills: {resume['skills']}")
    if resume.get("experience"):
        exp_str = "; ".join([f"{e.get('position','')} at {e.get('company','')} ({e.get('years','')})" for e in resume["experience"]])
        parts.append(f"Experience: {exp_str}")
    if resume.get("education"):
        edu = resume["education"]
        parts.append(f"Education: {edu.get('degree','')} at {edu.get('school','')}")
    if resume.get("keywords"):
        parts.append(f"Keywords: {resume['keywords']}")
    return "\n".join(parts)


def serialize_jd_json(jd: dict) -> str:
    parts = []
    if jd.get("title"):
        parts.append(f"Title: {jd['title']}")
    if jd.get("skills"):
        parts.append(f"Skills: {jd['skills']}")
    if jd.get("responsibilities"):
        parts.append("Responsibilities: " + "; ".join(jd["responsibilities"]))
    if jd.get("requirements"):
        parts.append("Requirements: " + "; ".join(jd["requirements"]))
    if jd.get("keywords"):
        parts.append(f"Keywords: {jd['keywords']}")
    return "\n".join(parts)

def main():
    # ===== é…ç½®è·¯å¾„ =====
    jd_file = "./data/job_description/jd1.txt"
    resumes_dir = "./data/resumes"
    cache_dir = os.environ.get("HF_HOME", None)

    print("ğŸ” Step 1: åŠ è½½èŒä½æè¿°å’Œç®€å†...")
    # ===== åŠ è½½åŸå§‹æ•°æ® =====
    job_desc = load_job_description(jd_file)
    resumes, resume_names, resume_pages = load_resumes(resumes_dir)
    print(f"âœ… åŠ è½½å®Œæˆ: èŒä½æè¿° 1 ä»½ï¼Œç®€å† {len(resumes)} ä»½")

    # ===== åˆå§‹åŒ– OpenAIProvider =====
    print("\nğŸ” Step 2: åˆå§‹åŒ– OpenAIProvider...")
    provider = OpenAIProvider(model_name="gpt-4o-mini")
    print("âœ… OpenAIProvider åˆå§‹åŒ–å®Œæˆ")

    # ===== æå–ç»“æ„åŒ– JSON =====
    print("\nğŸ” Step 3: æå–ç®€å†å’Œ JD çš„ JSON ä¿¡æ¯...")
    resume_jsons = []
    for i, text in enumerate(resumes):
        print(f"  -> è§£æç®€å† {i+1}/{len(resumes)}: {resume_names[i]}")
        rj = extract_with_openai(provider, text, RESUME_EXTRACTION_PROMPT, "resume")
        resume_jsons.append(rj)
    jd_json = extract_with_openai(provider, job_desc, JD_EXTRACTION_PROMPT, "jd")
    print("âœ… æå–å®Œæˆ")

    # ===== åºåˆ—åŒ– =====
    print("\nğŸ” Step 4: åºåˆ—åŒ– JSON ä¸ºæ–‡æœ¬...")
    jd_serialized = serialize_jd_json(jd_json)
    resumes_serialized = [serialize_resume_json(rj) for rj in resume_jsons]
    print("âœ… åºåˆ—åŒ–å®Œæˆ")

    # æ‰“å°æ ·ä¾‹
    print("\nğŸ“„ JD JSON ç¤ºä¾‹:")
    print(json.dumps(jd_json, indent=2, ensure_ascii=False))
    print("\nğŸ“„ ç¬¬ä¸€ä¸ª Resume JSON ç¤ºä¾‹:")
    print(json.dumps(resume_jsons[0], indent=2, ensure_ascii=False))

    # ===== æ’åº =====
    print("\nğŸ” Step 5: æ’åºå€™é€‰ç®€å†...")
    ranker = Ranker(model_cache_dir=cache_dir, device="cpu")
    ranker.load_models()
    results = ranker.rank_with_fields(
        jd_json,
        resume_jsons,   # æ‰€æœ‰ç®€å† JSON
        weights={"skills": 0.4, "experience": 0.5, "education": 0.1},
        top_k=min(30, len(resumes)),
    )
    print("âœ… æ’åºå®Œæˆ")

    # ===== è¾“å‡ºç»“æœ =====
    print("\nğŸ“Š Step 6: è¾“å‡ºç»“æœ")
    payload = []
    for i, r in enumerate(results):
        idx = r.resume_index
        fname = resume_names[idx]
        payload.append({
            "rank": i + 1,
            "file": fname,
            "final_score": r.final_score,
            "field_scores": r.field_scores,
            "meta": r.meta
        })

    print(json.dumps(payload, indent=2, ensure_ascii=False))
    print("\nğŸ‰ å…¨æµç¨‹å®Œæˆ")



if __name__ == "__main__":
    main()
